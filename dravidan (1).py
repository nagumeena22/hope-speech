# -*- coding: utf-8 -*-
"""Dravidan.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15ct7S5BcaGWpfJ7ItyArYH0SI_YcSzAk
"""

!pip install -q transformers datasets accelerate scikit-learn pandas emoji

import pandas as pd
import numpy as np
import re
import emoji
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer
)

CSV_PATH = "/content/train_CG.csv"
TEXT_COL = "Text"
LABEL_COL = "Label"

# smaller, faster model
MODEL_NAME = "distilbert-base-multilingual-cased"

# speed-oriented settings
MAX_LEN = 64        # shorter sequences
BATCH_SIZE = 8      # smaller batch for less GPU memory
NUM_EPOCHS = 3      # fewer epochs
LR = 3e-5           # a bit higher, to converge in fewer epochs

df = pd.read_csv(CSV_PATH)

print(df.head())
print(df[LABEL_COL].value_counts())

import emoji
import re

url_pattern = re.compile(r"http\S+|www\.\S+")
user_pattern = re.compile(r"@\w+")
space_pattern = re.compile(r"\s+")

def clean_text(text):
    if not isinstance(text, str):
        return ""
    text = text.strip()
    text = url_pattern.sub("<URL>", text)
    text = user_pattern.sub("@USER", text)

    text = emoji.replace_emoji(text, replace=" ")
    text = space_pattern.sub(" ", text)
    return text

df["clean_text"] = df[TEXT_COL].astype(str).apply(clean_text)

# Standardize label strings (if needed)
df[LABEL_COL] = df[LABEL_COL].str.lower().str.strip()

# Map labels to ids
label_encoder = LabelEncoder()
df["label_id"] = label_encoder.fit_transform(df[LABEL_COL])

id2label = {i: l for i, l in enumerate(label_encoder.classes_)}
label2id = {l: i for i, l in id2label.items()}
num_labels = len(id2label)
print(id2label)

# Train/validation split
train_df, val_df = train_test_split(
    df,
    test_size=0.1,
    random_state=42,
    stratify=df["label_id"]
)

print(len(train_df), len(val_df))

df[LABEL_COL] = df[LABEL_COL].str.lower().str.strip()


label_encoder = LabelEncoder()
df["label_id"] = label_encoder.fit_transform(df[LABEL_COL])

id2label = {i: l for i, l in enumerate(label_encoder.classes_)}
label2id = {l: i for i, l in id2label.items()}
num_labels = len(id2label)
print(id2label)


train_df, val_df = train_test_split(
    df,
    test_size=0.1,
    random_state=42,
    stratify=df["label_id"]
)

print(len(train_df), len(val_df))

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

class TuluHopeDataset(Dataset):
    def __init__(self, df, text_col, label_col_id):
        self.texts = df[text_col].tolist()
        self.labels = df[label_col_id].tolist()

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        label = self.labels[idx]

        enc = tokenizer(
            text,
            truncation=True,
            padding="max_length",
            max_length=MAX_LEN,
            return_tensors="pt"
        )

        item = {k: v.squeeze(0) for k, v in enc.items()}
        item["labels"] = torch.tensor(label, dtype=torch.long)
        return item

train_dataset = TuluHopeDataset(train_df, "clean_text", "label_id")
val_dataset   = TuluHopeDataset(val_df,   "clean_text", "label_id")

from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="/content/tulu_hope_ckpt",
    do_train=True,
    do_eval=True,
    learning_rate=LR,
    per_device_train_batch_size=BATCH_SIZE,
    per_device_eval_batch_size=BATCH_SIZE,
    num_train_epochs=NUM_EPOCHS,
    weight_decay=0.01,
    logging_steps=50,
    logging_dir="/content/logs",
    report_to="none",
    fp16=True  # Enable mixed-precision training for faster GPU performance
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    # Changed 'tokenizer' to 'processing_class' to address FutureWarning
    processing_class=tokenizer,
    compute_metrics=compute_metrics
)

trainer.train()
trainer.evaluate()

model = AutoModelForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=num_labels,
    id2label=id2label,
    label2id=label2id
)

from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
import numpy as np

def compute_metrics(p):
    predictions, labels = p
    predictions = np.argmax(predictions, axis=1)

    accuracy  = accuracy_score(labels, predictions)
    f1_macro  = f1_score(labels, predictions, average='macro')
    precision = precision_score(labels, predictions, average='macro')
    recall    = recall_score(labels, predictions, average='macro')

    return {
        "accuracy": accuracy,
        "f1_macro": f1_macro,
        "precision": precision,
        "recall": recall
    }

import pandas as pd
import numpy as np
import torch
from torch.utils.data import Dataset

# 1. Load your test file
TEST_PATH = "/content/test_data_withoutlabelCG.csv"   # change path/name
TEXT_COL = "Text"

test_df = pd.read_csv(TEST_PATH)
test_df["clean_text"] = test_df[TEXT_COL].astype(str).apply(clean_text)

class TuluHopeTestDataset(Dataset):
    def __init__(self, df, text_col):
        self.texts = df[text_col].tolist()

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        enc = tokenizer(
            text,
            truncation=True,
            padding="max_length",
            max_length=MAX_LEN,
            return_tensors="pt"
        )
        return {k: v.squeeze(0) for k, v in enc.items()}

test_dataset = TuluHopeTestDataset(test_df, "clean_text")

pred_outputs = trainer.predict(test_dataset)
pred_ids = np.argmax(pred_outputs.predictions, axis=-1)


pred_labels = [id2label[i] for i in pred_ids]

submission = pd.DataFrame({
    "labels": pred_labels
})

submission.to_csv("predictions.csv", index=False)
print(submission.head())

